---
title: "EDA_GroupG"
output: html_document
date: "2025-10-13"
Authors: "Oksana Efron, Sabrina Hassan, Keri Gagnow"
---

```{r setup, include=FALSE}
library(readr)
library(psych)
library(corrplot)
library(ggplot2)
```
Importing our dataset into R Markdown:
```{r}
library(readr)

student_depression_dataset_CSV <- read_csv("student_depression_dataset_CSV.csv")

View(student_depression_dataset_CSV)
```
We found a data set that analyzes mental health trends and predictors among students, which we ended up naming to StudentDep. We reorganized the data so it has 70 cases and 17 variables. Our goal was to make sure that the variables was layed out properly and in a better fit so we could analyze.

```{r}
StudentDep <- student_depression_dataset_CSV[1:70, c("Gender", "Age", "City", "Profession", "Academic_Pressure", "Work_Pressure", "CGPA", "Study_Satisfaction", "Job_Satisfaction", "Sleep_Duration", "Dietary_Habits", "Degree", "Suicidal_Thoughts", "Work/Study_Hours", "Financial_Stress", "Family_History_of_Mental_Illness", "Depression")]
dim(StudentDep)
```

```{r}
StudentDep
```
What we did here is converted the categorical variables into numerical form, such as 1 for males and 0 for females. As well as for Dietary Habits and Sleep Duration, they were changed into numeric factors.

```{r}
# Making sure to change Gender to binary so its Males = 1, and Female = 0
StudentDep$Gender <- ifelse(StudentDep$Gender == "Male", 1, 0)
 
# Also doing this for Family History of Mental Illness converting, binary (Yes = 1, No = 0)
StudentDep$Family_History_of_Mental_Illness <- ifelse(StudentDep$Family_History_of_Mental_Illness == "Yes", 1, 0)
 
# Converting ordinal variables for Dietary Habits and Sleep Duration
StudentDep$Dietary_Habits <- as.numeric(factor(StudentDep$Dietary_Habits,
                                                  levels = c("Unhealthy", "Moderate", "Healthy"),
                                                  ordered = TRUE))
StudentDep$Sleep_Duration <- as.numeric(factor(StudentDep$Sleep_Duration,
                                                  levels = c("'Less than 5 hours'", "'5-6 hours'", "'7-8 hours'", "'More than 8 hours'"),
                                                  ordered = TRUE))
 
# Converting Depression, for binary levels. Yes = 1 and No = 0
#StudentDep$Depression <- ifelse(StudentDep$Depression == "Yes", 1, 0)
 
str(StudentDep)


```
```{r}
summary(StudentDep)
```
```{r}
dim(StudentDep)
```
We used boxplot to compare Academic Pressure and Study Satisfaction in  One-Dimensional Graphs
```{r}
ggplot(StudentDep) + geom_bar(aes(x = Academic_Pressure))

ggplot(StudentDep) + geom_bar(aes(x = Study_Satisfaction))
```




For here we made sure to have variables that are predictors, and variables that are response variables.

```{r}
# Predictor Variables
X <- StudentDep[, c("Gender", "Age", "Academic_Pressure", 
                    "CGPA", "Dietary_Habits", "Financial_Stress",
                    "Family_History_of_Mental_Illness")]
# Response Variables
Y <- StudentDep[, c("Depression", "Study_Satisfaction", "Sleep_Duration")]
```
Used summary instead of describe()

```{r}
# Defining skewness and kurtosis
Skewness <- function(x) {
  n <- length(x)
  m3 <- sum((x - mean(x))^3)/n
  s3 <- sd(x)^3
}

kurtosis <- function(x) {
  n <- length(x)
  m4 <- sum((x - mean(x))^4)/n
  s4 <- sd(x)^4
  m4/s4
}


summary(StudentDep)
```
```{r}
# Calculating mean, Standard Deviation, Skewness, and Kurtosis
Vals <- sapply(StudentDep, is.numeric)
Data <- StudentDep[, Vals]

# Removing NA
Data <- Data[, colSums(is.na(Data)) < nrow(Data)]

SumStats <- data.frame(
  variable = colnames(Data),
  Mean = sapply(Data, mean),
  SD = sapply(Data, sd),
  Skewness = sapply(Data, Skewness),
  Kurtosis = sapply(Data, kurtosis)
)
print(SumStats)
```
Finding Covariance, Correlation, Scatter Plot Matrices. A issue faced would be that R is taking binary number that uses for example; Yes = 1, No = 0, into the correlation/covariance which is causing NA to be put into the dataset.

```{r}
# Covariance Matrix
Covar <- cov(Data)
print("Covariance Matrix:")
Covar
```

```{r}
# Correlation Matrix
Corr <- cor(Data, use = "pairwise.complete.obs")
print("Correlation Matrix:")
Corr
```

```{r}
# Scatter Plot
pairs(Data, pch = 19, main = "Scatter Plot")

```
```{r}
# Correlation Plot
corrplot(Corr,na.label = " ", method = "circle")
```
We noticed that there is a strong correlation along the main diagonal of the correlation plot

```{r}
pairs(Data, pch = 19, main = "Scatter matrix for outliers")
```

```{r}
# Mahalanobis distance
Vals <- sapply(Data, is.numeric)
Data <- Data[, Vals]

# Remove
Data <- na.omit(Data)

# Remove columns that have zero variance
Data <- Data[, apply(Data, 2, var) != 0]

center <- colMeans(Data)
Covar <- cov(Data)

mahalDist <- mahalanobis(Data, center, Covar)
Data$mahalanobis <- mahalDist


# Identify Outliers
CutOff <- qchisq(0.975, df = ncol(Data) - 1)
Outliers <- which(mahalDist > CutOff)
Outliers
```

When using the Mahalanobis distance and the use of scattterplots, we wanted to observe the correlation matrix and the outliers. We saw that certain variables may be weaker or stronger which depends between the variables. Also, can see that some extreme data points misshaped the strengths as well the binary variables caused the dataset to not look as appealing.



```{r}
# Predictor Variables
X <- StudentDep[, c("Gender", "Age", "Academic_Pressure", "CGPA", "Dietary_Habits", "Financial_Stress", "Family_History_of_Mental_Illness")]

# Response Variables
Y <- StudentDep[, c("Depression", "Study_Satisfaction", "Job_Satisfaction", "Sleep_Duration")]

# Converting to numeric variables
X[] <- lapply(X, function(col) as.numeric((col)))
Y[] <- lapply(Y, function(col) as.numeric((col)))

# Removing rows with NA
X <- na.omit(X)
Y <- na.omit(Y)

# Keeping columns with zero variance
vars <- sapply(X, var, na.rm = TRUE)
X <- X[, vars > 0]
varsY <- sapply(Y, var, na.rm = TRUE)
Y <- Y[, !is.na(varsY) & varsY > 0]

Y

# Correlation matrices
RX <- cor(X, use = "pairwise.complete.obs")
RY <- cor(Y, use = "pairwise.complete.obs")



# Eigenvalues
EigX <- eigen(RX)
EigY <- eigen(RY)

EigX$values
EigY$values




```

We will be using the Principal Component Analysis to see if an oblique or orthogonal rotation will be necessary for this data set. We will be setting our intrinsic dimensionality to be 3 for both the predictors and response variables
```{r}
R <- pca(r = RX, nfactors = 3, rotate = "varimax")$loadings[]
R1 <- pca(r = RY, nfactors = 3, rotate = "varimax")$loadings[]
print("Orthogonal Rotation :")
R
R1

R_1 <- pca(r = RX, nfactors = 3, rotate = "oblimin")$loadings[]
R_2 <- pca(r = RY, nfactors = 3, rotate = "oblimin")$loadings[]

print("Oblique Rotation :")
R_1
R_2
```
After further analysis of both rotations, we have determined that Orthogonal Rotation will be sufficient in this scenario as the Oblique rotation made the dimensions for the response variables unnecessarily more complex, which the end goal was to reduce the number of complex dimensions with orthogonal or oblique rotation
We will also use correlation plots to visually identify complex dimensions with our rotated loading matrices:
```{r}
corrplot(R, method = "circle")


corrplot(R1, method = "circle")


corrplot(R_1, method = "circle")
```
After visualizing the loading matrices and visualizing them with corrplots, we believe that oblique rotation would not be necessary for this particular data set as the orthogonal rotation and oblique rotation look exactly the same, we no notable differences with complex dimensions. With this in mind, we will be using orthogonal rotation for this data set
```{r}
#Variance 
Var <- (EigX$values / sum(EigX$values)) * 100
Var

CummulVar_f2 <- Var[1] + Var[2]
CummulVar_f2
# Scree Plot 
plot(EigX$values, type = "b", main = "Scree Plot", 
     xlab = "Component", ylab = "Eigenvalue")

# Communalities
communalities <- rowSums(R^2)
communalities
```

Interperting Variance and Covariance:
 We used PCA with a varimax rotation. This analysis gave us 2 main components that together explain 60.46% of the total variance from our four original variables: 
Age, Academic Pressure, CGPA, and Financial Stress. The first component, RC1, primarily represents Academic Pressure and Financial Stress. The second component, RC2, 
is mainly represented by Age and CGPA. We added a scree plot that also agrees with our loadings. The scree plot confirms that two components are optimal, as shown by the 
elbow after the second point where the eigenvalues begin to level off. By using these 2 components, we keep 60.46% of the original information, meaning about 39.54% is lost. 
While this isn't a huge amount of data loss, it is something to watch out for. The communalities tell us how well each variable is captured by the two components. We found 
that CGPA and Academic Pressure are represented the best, while Financial Stress and Age lose a bit more of their unique information. In short, the PCA simplified our four 
variables into two main themes: one for stress factors and one for academic performance.


